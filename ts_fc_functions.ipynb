{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.model_selection import StratifiedGroupKFold, StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from tabpfn import TabPFNClassifier\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import warnings\n",
    "from skopt import BayesSearchCV, space\n",
    "from sklearn.metrics import average_precision_score\n",
    "# use feature importance for feature selection\n",
    "from numpy import sort\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_selection import SelectFromModel    \n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import GridSearchCV, KFold, cross_val_score, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from mlxtend.classifier import EnsembleVoteClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from scipy.stats import uniform, randint\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "### Parameters\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from autogluon.core.metrics import make_scorer\n",
    "from autogluon.tabular import TabularPredictor\n",
    "from autogluon.common import space\n",
    "from autogluon.timeseries import TimeSeriesDataFrame, TimeSeriesPredictor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "##import ts_fc_functions as ff\n",
    "import import_ipynb\n",
    "from darts import TimeSeries\n",
    "from darts.models import NBEATSModel, XGBModel, TiDEModel, LightGBMModel, AutoARIMA\n",
    "from darts.dataprocessing.transformers import Scaler, MissingValuesFiller\n",
    "from darts.metrics import mape, r2_score, mae, rmse\n",
    "from darts.utils.likelihood_models import QuantileRegression\n",
    "import yaml\n",
    "from darts.utils.timeseries_generation import datetime_attribute_timeseries\n",
    "from darts.utils.likelihood_models import LaplaceLikelihood\n",
    "import random\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_ts(data,start_prediction,value_to_predict,min_value,persistence_use_case,persistence_time_lag):\n",
    "    if start_prediction == '0h':\n",
    "        time_lag = 'time_lag_2'\n",
    "    elif start_prediction == '24h':\n",
    "        time_lag = 'time_lag_1'\n",
    "    else:\n",
    "         time_lag = 'time_lag_target'\n",
    "\n",
    "    \n",
    "    data_sorted = data.sort_values(by=['encounter_id', 'recorded_time'])\n",
    "    # Identify the first occurrence where time_lag_target is True for each encounter_id\n",
    "    first_true = data_sorted[data_sorted[time_lag]].groupby('encounter_id').first().reset_index()\n",
    "    first_true = first_true[['encounter_id', 'recorded_time']]\n",
    "    # Identify the last occurrence where time_lag_target is True for each encounter_id\n",
    "    last_true = data_sorted[data_sorted['time_lag_target']].groupby('encounter_id').last().reset_index()\n",
    "    last_true = last_true[['encounter_id', 'recorded_time']]\n",
    "    # Merge to get the first true recorded_time back to the original DataFrame\n",
    "    data_with_first_true = pd.merge(data_sorted, first_true, on='encounter_id', suffixes=('', '_first_true'))\n",
    "    data_with_last_true = pd.merge(data_sorted, last_true, on='encounter_id', suffixes=('', '_last_true'))\n",
    "\n",
    "    # Flag rows to delete\n",
    "    data_with_first_true['delete_flag'] = (data_with_first_true['recorded_time'] > data_with_first_true['recorded_time_first_true']) & ~data_with_first_true[time_lag]\n",
    "    data_with_last_true['delete_flag'] = (data_with_last_true['recorded_time'] > data_with_last_true['recorded_time_last_true']) & ~data_with_last_true['time_lag_target']\n",
    "\n",
    "    # Filter out rows to delete\n",
    "    data = data_with_first_true[~data_with_first_true['delete_flag']].drop(columns=['delete_flag', 'recorded_time_first_true'])\n",
    "    # Ensure 'value' is not missing and 'time_lag_1' is properly boolean\n",
    "    data = data.dropna(subset=[value_to_predict])  # Drop rows where 'value' is missing if necessary\n",
    "    data_last = data_with_last_true[~data_with_last_true['delete_flag']].drop(columns=['delete_flag', 'recorded_time_last_true'])\n",
    "    data_last = data_last.dropna(subset=[value_to_predict])  # Drop rows where 'value' is missing if necessary\n",
    "\n",
    "    # Filter DataFrame to include only rows where time_lag_1 is True\n",
    "    # Identify encounter_ids where time_lag_target is True at least once\n",
    "    condition_1_encounter_ids = data[data[time_lag]].groupby('encounter_id').size().index\n",
    "    condition_1_encounter_ids_last = data_last[data_last[time_lag]].groupby('encounter_id').size().index\n",
    "\n",
    "    if persistence_use_case == True:\n",
    "        time_lag_1_true_df = data[data[persistence_time_lag]]\n",
    "        time_lag_1_true_df_last = data_last[data_last[persistence_time_lag]]\n",
    "\n",
    "        # Check the maximum value where time_lag_1 is True for each group\n",
    "        max_values = time_lag_1_true_df.groupby('encounter_id')[value_to_predict].max()\n",
    "        # Identify encounter_ids where the maximum value is >= 38\n",
    "        condition_2_encounter_ids = max_values[max_values >= min_value].index\n",
    "        # Combine both conditions\n",
    "        final_encounter_ids = set(condition_1_encounter_ids) & set(condition_2_encounter_ids)\n",
    "        # last\n",
    "        time_lag_1_last_df = data_last[data_last[persistence_time_lag]]\n",
    "        # Check the maximum value where time_lag_1 is True for each group\n",
    "        max_values = time_lag_1_true_df_last.groupby('encounter_id')[value_to_predict].max()\n",
    "        # Identify encounter_ids where the maximum value is >= 38\n",
    "        condition_2_encounter_ids = max_values[max_values >= min_value].index\n",
    "        # Combine both conditions\n",
    "        final_encounter_ids_full = set(condition_1_encounter_ids_last) & set(condition_2_encounter_ids)\n",
    "    else:\n",
    "    # Filter the data based on the final_encounter_ids\n",
    "        final_encounter_ids = set(condition_1_encounter_ids)\n",
    "        final_encounter_ids_full = set(condition_1_encounter_ids_last)\n",
    "\n",
    "    filtered_data = data[data['encounter_id'].isin(final_encounter_ids)]\n",
    "    filtered_data_full = data_last[data_last['encounter_id'].isin(final_encounter_ids_full)]\n",
    "\n",
    "    return filtered_data, filtered_data_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_station(filtered_data,station):\n",
    "    station_ = 'station_'+station\n",
    "    df_stations = pd.read_csv(\"./data/station_groups_new.csv\")\n",
    "    df_stations = df_stations.drop_duplicates()\n",
    "    df_stations = pd.get_dummies(df_stations, columns=[\"station_group\"], prefix=\"station\")\n",
    "    grouped = df_stations.groupby('encounter_id')\n",
    "    df_stations = grouped.max()\n",
    "    df_stations.reset_index(inplace=True)\n",
    "    df = filtered_data.merge(df_stations, on='encounter_id', how='left')\n",
    "    df.drop(columns=['subject_reference_y'], inplace=True)\n",
    "    df.rename(columns={'subject_reference_x': 'subject_reference'}, inplace=True)\n",
    "    df = df[df[station_] == True]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def resample_ts(df,resample_rate,min_ts_length, imputation,max_train_length,value_to_predict,covariates):\n",
    "    df['recorded_time'] = pd.to_datetime(df['recorded_time'])\n",
    "    # Define the aggregation functions for numerical columns\n",
    "    if covariates == True:\n",
    "        agg_funcs = {\n",
    "            'value': 'max',\n",
    "            'crp': 'max',\n",
    "            'heart_rate': 'max',\n",
    "            'mean_arterial_pressure': 'max',\n",
    "            'so2': 'max',\n",
    "            'bili': 'max',\n",
    "            'ggt': 'max',\n",
    "            'hb': 'max',\n",
    "            'krea': 'max',\n",
    "            'leua': 'max',\n",
    "            #'fourier_mean_magnitude': 'max',\n",
    "            #'fourier_std_dev_magnitude': 'max',\n",
    "            #'fourier_max_magnitude': 'max',\n",
    "            #'fourier_min_magnitude': 'max'\n",
    "        }\n",
    "    else:\n",
    "        agg_funcs = {\n",
    "            'crp': 'max'\n",
    "            #'fourier_mean_magnitude': 'max',\n",
    "            #'fourier_std_dev_magnitude': 'max',\n",
    "            #'fourier_max_magnitude': 'max',\n",
    "            #'fourier_min_magnitude': 'max'\n",
    "        }\n",
    "    \n",
    "    # Define a function to resample, aggregate, and interpolate within each group\n",
    "    def resample_aggregate_and_interpolate(group):\n",
    "        group = group.set_index('recorded_time')\n",
    "        resampled = group.resample(resample_rate).agg(agg_funcs)\n",
    "        resampled['encounter_id'] = group['encounter_id'].iloc[0]  # Add encounter_id back\n",
    "        resampled = resampled.reset_index()\n",
    "        \n",
    "        if imputation == 'interpolate':\n",
    "            # Interpolate missing values linearly within each group\n",
    "            resampled = resampled.groupby('encounter_id').apply(\n",
    "                lambda x: x.interpolate(method='linear', limit_direction='forward')\n",
    "            ).reset_index(drop=True)\n",
    "        elif imputation == 'forward_fill':\n",
    "                # Forward fill missing values within each group\n",
    "            resampled = resampled.groupby('encounter_id').apply(\n",
    "                lambda x: x.ffill()\n",
    "            ).reset_index(drop=True)\n",
    "            resampled = resampled.groupby('encounter_id').apply(\n",
    "                lambda x: x.bfill()\n",
    "            ).reset_index(drop=True)\n",
    "            resampled = resampled.fillna(0)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        return resampled\n",
    "    \n",
    "    # Apply the function to each group and combine results\n",
    "    resampled_df = df.groupby('encounter_id').apply(resample_aggregate_and_interpolate).reset_index(drop=True)\n",
    "\n",
    "    data_sorted = resampled_df.sort_values(by=['encounter_id', 'recorded_time'])\n",
    "    \n",
    "    # Group by 'encounter_id' and get the last 10 rows for each group\n",
    "    data_recent = data_sorted.groupby('encounter_id').tail(max_train_length)\n",
    "\n",
    "    # Reset index if needed\n",
    "    resampled_df = data_recent.reset_index(drop=True)\n",
    "    resampled_df = resampled_df[resampled_df[value_to_predict].notna()]\n",
    "\n",
    "    #resampled_df = resampled_df.dropna(subset=['crp'])\n",
    "    # Filter out encounter_ids with less than n data points\n",
    "    valid_encounters = resampled_df['encounter_id'].value_counts()\n",
    "    valid_encounters = valid_encounters[valid_encounters >= min_ts_length].index\n",
    "    resampled_df = resampled_df[resampled_df['encounter_id'].isin(valid_encounters)]\n",
    "\n",
    "    return resampled_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(resampled_df,prediction_length):\n",
    "    # Split the data into train and test sets\n",
    "    def split_train_test(group):\n",
    "        test_rows = group.nlargest(prediction_length, 'recorded_time')\n",
    "        train_rows = group.drop(test_rows.index)\n",
    "        return train_rows, test_rows\n",
    "    \n",
    "    train_list = []\n",
    "    test_list = []\n",
    "    \n",
    "    # Group by encounter_id and split each group\n",
    "    for name, group in resampled_df.groupby('encounter_id'):\n",
    "        train_rows, test_rows = split_train_test(group)\n",
    "        train_list.append(train_rows)\n",
    "        test_list.append(test_rows)\n",
    "    \n",
    "    # Concatenate the lists into DataFrames\n",
    "    train_data = pd.concat(train_list).reset_index(drop=True)\n",
    "    test_data = pd.concat(test_list).reset_index(drop=True)\n",
    "    return train_data, test_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cross_validate_model(df,prediction_length,data,value_to_predict,resample_rate,metric):\n",
    "    n_splits=5\n",
    "    num_trials=1\n",
    "    unique_encounter_ids = df['encounter_id'].unique()\n",
    "    # Perform multiple trials\n",
    "    for trial in range(num_trials):\n",
    "        kf = KFold(n_splits=n_splits, shuffle=True, random_state=trial)\n",
    "        # Split data for this trial\n",
    "        for train_index, val_index in kf.split(unique_encounter_ids):\n",
    "            train_encounter_ids = unique_encounter_ids[train_index]\n",
    "            val_encounter_ids = unique_encounter_ids[val_index]\n",
    "            \n",
    "            # Select rows based on encounter_id\n",
    "            training = df[df['encounter_id'].isin(train_encounter_ids)]\n",
    "            validation = df[df['encounter_id'].isin(val_encounter_ids)]\n",
    "\n",
    "            train_data_df, test_data_df = train_test(validation,prediction_length)\n",
    "            train_data_df_, test_data_df_ = train_test(training,prediction_length)\n",
    "            #return train_data_df, test_data_df,train_data_df_, test_data_df_\n",
    "            #predictor, train_data, test_data = train_model(train_data_df_,test_data_df_)\n",
    "            predictor, train_data, test_data = train_model(resampled_df, prediction_length, value_to_predict, resample_rate,metric)\n",
    "            train_data_ = df[['encounter_id', 'recorded_time', value_to_predict, 'value','bili','leua','krea','ggt','heart_rate','mean_arterial_pressure','so2']]\n",
    "            train_data_['recorded_time'] = train_data_['recorded_time'].dt.tz_localize(None)\n",
    "            test_data_ = test_data_df[['encounter_id', 'recorded_time', value_to_predict, 'value','bili','leua','krea','ggt','heart_rate','mean_arterial_pressure','so2']]\n",
    "            test_data_['recorded_time'] = test_data_['recorded_time'].dt.tz_localize(None)\n",
    "            data['recorded_time'] = pd.to_datetime(data['recorded_time'])\n",
    "            validation['recorded_time'] = validation['recorded_time'].dt.tz_localize(None)\n",
    "\n",
    "            \n",
    "            train_data = TimeSeriesDataFrame.from_data_frame(\n",
    "                train_data_,\n",
    "                id_column=\"encounter_id\",\n",
    "                timestamp_column=\"recorded_time\"\n",
    "            )\n",
    "            test_data = TimeSeriesDataFrame.from_data_frame(\n",
    "                test_data_,\n",
    "                id_column=\"encounter_id\",\n",
    "                timestamp_column=\"recorded_time\"\n",
    "            )\n",
    "            validation_data_ = TimeSeriesDataFrame.from_data_frame(\n",
    "                validation,\n",
    "                id_column=\"encounter_id\",\n",
    "                timestamp_column=\"recorded_time\"\n",
    "            )\n",
    "\n",
    "            \n",
    "            \n",
    "            predictions = predictor.predict(train_data,model='RecursiveTabular')\n",
    "            return train_data, test_data, predictions, predictor, train_data_df,  test_data_df,validation_data_,training,validation,train_data_df_,test_data_df_\n",
    "            \n",
    "            \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(resampled_df, prediction_length, value_to_predict, resample_rate,metric):\n",
    "    train_data_df, test_data_df = train_test(resampled_df,prediction_length)\n",
    "    #train_data_ = train_data_df[['encounter_id', 'recorded_time', value_to_predict, 'value','bili','leua','krea','ggt','heart_rate','mean_arterial_pressure','so2']]\n",
    "    train_data_ = train_data_df[['encounter_id', 'recorded_time', value_to_predict]]\n",
    "\n",
    "    train_data_['recorded_time'] = train_data_['recorded_time'].dt.tz_localize(None)\n",
    "    #test_data_ = resampled_df[['encounter_id', 'recorded_time', value_to_predict, 'value','bili','leua','krea','ggt','heart_rate','mean_arterial_pressure','so2']]\n",
    "    test_data_ = resampled_df[['encounter_id', 'recorded_time', value_to_predict]]\n",
    "\n",
    "    test_data_['recorded_time'] = resampled_df['recorded_time'].dt.tz_localize(None)\n",
    "    train_data = TimeSeriesDataFrame.from_data_frame(\n",
    "        train_data_,\n",
    "        id_column=\"encounter_id\",\n",
    "        timestamp_column=\"recorded_time\"\n",
    "    )\n",
    "    test_data = TimeSeriesDataFrame.from_data_frame(\n",
    "        test_data_,\n",
    "        id_column=\"encounter_id\",\n",
    "        timestamp_column=\"recorded_time\"\n",
    "    )\n",
    "    predictor = TimeSeriesPredictor(\n",
    "    prediction_length=prediction_length,\n",
    "    path=\"autogluon\",\n",
    "    target=value_to_predict,\n",
    "    eval_metric=metric,\n",
    "    freq=resample_rate,\n",
    "    verbosity=3,\n",
    "    quantile_levels = [0.05,0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95],\n",
    "    #known_covariates_names=['value','bili','leua','krea','ggt','heart_rate','mean_arterial_pressure','so2']\n",
    "\n",
    ")\n",
    "\n",
    "    \n",
    "    predictor.fit(\n",
    "        train_data,\n",
    "        presets=\"best_quality\",\n",
    "        #presets=\"chronos_ensemble\",\n",
    "        time_limit=600,\n",
    "        hyperparameters={\n",
    "        #\"DirectTabularModel\": {\"tabular_hyperparameters\": {\"XGB\":{} }},\n",
    "        #\"RecursiveTabularModel\": {\"tabular_hyperparameters\": {\"XGB\":{} }},\n",
    "        #\"AverageModel\":{},\n",
    "        #\"AutoARIMA\":{},\n",
    "        \"TiDE\":{},\n",
    "        \"DeepAR\": {},\n",
    "        \"PatchTSTModel\": {},\n",
    "        #\"WaveNetModel\": {},\n",
    "        \"TemporalFusionTransformerModel\": {},\n",
    "        \"DLinearModel\":{},\n",
    "        #            \"hidden_size\": space.Int(20, 100),\n",
    "        #            \"dropout_rate\": space.Categorical(0.1, 0.3)\n",
    "        #},     \n",
    "        #NBeatsCustomModel: {},    \n",
    "        #\"DeepAR\":{},\n",
    "        #\"Chronos\": {\n",
    "        #    \"model_path\": \"small\",\n",
    "        #    \"batch_size\": 64,\n",
    "        #\"optimization_strategy\": \"openvino\",\n",
    "        #},\n",
    "        #\"Chronos\": {            \n",
    "        #        \"model_path\": \"tiny\",\n",
    "        #        \"batch_size\": 64,\n",
    "        #        \"device\": \"cpu\"},\n",
    "\n",
    "       },\n",
    "    hyperparameter_tune_kwargs=None,\n",
    "    enable_ensemble=True,\n",
    "    )\n",
    "    return predictor, train_data, test_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ts_dictionary(resampled_df, prediction_length):\n",
    "    def remove_timezone_and_set_index(df):\n",
    "        df['recorded_time'] = df['recorded_time'].dt.tz_localize(None)\n",
    "        return df.set_index('recorded_time')\n",
    "\n",
    "    def create_timeseries_dict(grouped_data):\n",
    "        series_dict = {}\n",
    "        for encounter_id, group in grouped_data:\n",
    "            group = group.sort_values(by='recorded_time')\n",
    "            ts_series = TimeSeries.from_dataframe(group, value_cols='crp', freq='D')\n",
    "            series_dict[encounter_id] = ts_series\n",
    "        return series_dict\n",
    "\n",
    "    train_data_df, test_data_df = train_test(resampled_df, prediction_length)\n",
    "    train_data_df = remove_timezone_and_set_index(train_data_df)\n",
    "    test_data_df = remove_timezone_and_set_index(test_data_df)\n",
    "\n",
    "    resampled_df = resampled_df.reset_index()\n",
    "    resampled_df = remove_timezone_and_set_index(resampled_df)\n",
    "\n",
    "    grouped_train = train_data_df.groupby('encounter_id')\n",
    "    grouped_test = test_data_df.groupby('encounter_id')\n",
    "    grouped_validation = resampled_df.groupby('encounter_id')\n",
    "\n",
    "    train_series_dict = create_timeseries_dict(grouped_train)\n",
    "    test_series_dict = create_timeseries_dict(grouped_test)\n",
    "    valid_series_dict = create_timeseries_dict(grouped_validation)\n",
    "\n",
    "    return valid_series_dict, test_series_dict, train_series_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ts_dictionary_covariates(resampled_df, prediction_length,covariates_list):\n",
    "    def remove_timezone_and_set_index(df):\n",
    "        df['recorded_time'] = df['recorded_time'].dt.tz_localize(None)\n",
    "        return df.set_index('recorded_time')\n",
    "\n",
    "    def create_timeseries_dict(grouped_data):\n",
    "        series_dict = {}\n",
    "        for encounter_id, group in grouped_data:\n",
    "            group = group.sort_values(by='recorded_time')\n",
    "            ts_series = TimeSeries.from_dataframe(group, value_cols=covariates_list, freq='D')\n",
    "            series_dict[encounter_id] = ts_series\n",
    "        return series_dict\n",
    "\n",
    "    train_data_df, test_data_df = train_test(resampled_df, prediction_length)\n",
    "    train_data_df = remove_timezone_and_set_index(train_data_df)\n",
    "    test_data_df = remove_timezone_and_set_index(test_data_df)\n",
    "\n",
    "    resampled_df = resampled_df.reset_index()\n",
    "    resampled_df = remove_timezone_and_set_index(resampled_df)\n",
    "\n",
    "    grouped_train = train_data_df.groupby('encounter_id')\n",
    "    grouped_test = test_data_df.groupby('encounter_id')\n",
    "    grouped_validation = resampled_df.groupby('encounter_id')\n",
    "\n",
    "    train_series_dict = create_timeseries_dict(grouped_train)\n",
    "    test_series_dict = create_timeseries_dict(grouped_test)\n",
    "    valid_series_dict = create_timeseries_dict(grouped_validation)\n",
    "\n",
    "    return valid_series_dict, test_series_dict, train_series_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_train_validation(train_series_dict, split_ratio=0.8):\n",
    "    \"\"\"\n",
    "    Splits each TimeSeries in train_series_dict into training and validation series.\n",
    "    \n",
    "    Args:\n",
    "        train_series_dict (dict): A dictionary with keys as identifiers and values as TimeSeries objects.\n",
    "        split_ratio (float): The ratio of data to use for training. The rest will be used for validation.\n",
    "\n",
    "    Returns:\n",
    "        dict, dict: Two dictionaries containing the training and validation series.\n",
    "    \"\"\"\n",
    "    train_split_dict = {}\n",
    "    val_split_dict = {}\n",
    "\n",
    "    for key, series in train_series_dict.items():\n",
    "        # Calculate the split point\n",
    "        split_point = int(len(series) * split_ratio)\n",
    "        \n",
    "        # Split the series\n",
    "        train_series, val_series = series.split_before(series.time_index[split_point])\n",
    "        \n",
    "        # Store the results\n",
    "        train_split_dict[key] = train_series\n",
    "        val_split_dict[key] = val_series\n",
    "    \n",
    "    return train_split_dict, val_split_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def pad_time_series(ts, max_length, pad_value=0):\n",
    "    \"\"\"\n",
    "    Pads a TimeSeries object along its time dimension to max_length with the pad_value,\n",
    "    extending the time series into the past.\n",
    "\n",
    "    Args:\n",
    "        ts (TimeSeries): The TimeSeries object to pad.\n",
    "        max_length (int): The length to pad the time series to.\n",
    "        pad_value (numeric): The value used for padding.\n",
    "\n",
    "    Returns:\n",
    "        TimeSeries: A new TimeSeries object with padded data.\n",
    "    \"\"\"\n",
    "    # Convert TimeSeries to pandas DataFrame\n",
    "    df = ts.pd_dataframe()\n",
    "    \n",
    "    # Extract current length and time index\n",
    "    current_length = df.shape[0]\n",
    "    time_index = df.index\n",
    "    \n",
    "    if current_length >= max_length:\n",
    "        raise ValueError(f\"Current length of time series is already >= max_length.\")\n",
    "    \n",
    "    # Compute padding\n",
    "    padding_length = max_length - current_length\n",
    "    \n",
    "    # Create new time index for padded data\n",
    "    start_time = time_index[0]\n",
    "    new_time_index = pd.date_range(start=start_time - pd.Timedelta(days=padding_length), periods=padding_length, freq='D')\n",
    "    \n",
    "    # Pad the data along the time dimension (prepend padding)\n",
    "    padded_data = np.pad(df.values, ((padding_length, 0), (0, 0)), 'constant', constant_values=pad_value)\n",
    "    \n",
    "    # Create new DataFrame with padded data\n",
    "    padded_df = pd.DataFrame(padded_data, columns=df.columns, index=new_time_index.append(time_index))\n",
    "    \n",
    "    # Convert the padded DataFrame back to TimeSeries\n",
    "    padded_ts = TimeSeries.from_dataframe(padded_df)\n",
    "    \n",
    "    return padded_ts\n",
    "\n",
    "def pad_series_dict(series_dict, max_length, pad_value=0):\n",
    "    \"\"\"\n",
    "    Pads each TimeSeries in the dictionary to the max_length with the pad_value.\n",
    "\n",
    "    Args:\n",
    "        series_dict (dict): A dictionary where the keys are series identifiers\n",
    "                            and the values are TimeSeries objects.\n",
    "        max_length (int): The length to pad each series to.\n",
    "        pad_value (numeric): The value used for padding.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary of padded TimeSeries.\n",
    "    \"\"\"\n",
    "    padded_series_dict = {}\n",
    "    for key, ts in series_dict.items():\n",
    "        padded_series = pad_time_series(ts, max_length, pad_value)\n",
    "        padded_series_dict[key] = padded_series\n",
    "    return padded_series_dict\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Now padded_series_dict contains the padded TimeSeries objects\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model_vanilla(model,train_series_dict,valid_series_dict_full,covariates,train_series_covars):\n",
    "    if covariates == True:\n",
    "        model.fit(series=list(train_series_dict.values()),past_covariates=list(train_series_covars.values()),val_series=list(valid_series_dict_full.values()))\n",
    "        \n",
    "    else:\n",
    "        model.fit(series=list(train_series_dict.values()))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_stats_model_vanilla(model,train_series_dict,valid_series_dict_full,covariates,train_series_covars):\n",
    "    if covariates == True:\n",
    "        model.fit(series=list(train_series_dict.values()),past_covariates=list(train_series_covars.values()), val_series=list(valid_series_dict_full.values()))\n",
    "        \n",
    "    else:\n",
    "        model.fit(series=list(train_series_dict.values()),val_series=list(valid_series_dict_full.values()))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_configs(min_train_length,prediction_length,max_train_length,padding):\n",
    "    if padding == True:\n",
    "        training = max_train_length-prediction_length\n",
    "    else:\n",
    "        training = min_train_length-prediction_length\n",
    "\n",
    "    #\n",
    "    model_nbeats = NBEATSModel(\n",
    "    input_chunk_length=training,\n",
    "    output_chunk_length=prediction_length,\n",
    "    generic_architecture=True,\n",
    "    num_stacks=10,\n",
    "    num_blocks=1,\n",
    "    num_layers=4,\n",
    "    layer_widths=512,\n",
    "    n_epochs=500,\n",
    "    nr_epochs_val_period=1,\n",
    "    batch_size=800,\n",
    "    likelihood=LaplaceLikelihood()\n",
    "    )\n",
    "    #\n",
    "    model_xgb = XGBModel(\n",
    "    lags=training,\n",
    "    output_chunk_length=prediction_length, \n",
    "    force_col_wise='true',\n",
    "    use_static_covariates=False,\n",
    "    multi_models=True)\n",
    "    #\n",
    "    model_lgb = LightGBMModel(\n",
    "    lags=training,\n",
    "    output_chunk_length=prediction_length, \n",
    "    force_col_wise='true',\n",
    "    use_static_covariates=False,\n",
    "    likelihood=\"quantile\",\n",
    "    quantiles=[0.05, 0.1, 0.5, 0.9, 0.95],\n",
    "    #quantiles=[0.49, 0.5, 0.51],\n",
    "\n",
    "    multi_models=False)\n",
    "    #\n",
    "    model_tide = TiDEModel(\n",
    "    input_chunk_length=training,\n",
    "    output_chunk_length=prediction_length\n",
    "    )\n",
    "    model_aarima = AutoARIMA()\n",
    "    return model_nbeats, model_xgb, model_lgb, model_tide, model_aarima\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_old(model,train_series_dict,valid_series_dict,covariates,train_series_covars):\n",
    "    if covariates == True:\n",
    "        pred = model.predict(n=1, series=list(train_series_dict.values()), past_covariates=list(train_series_covars.values()))\n",
    "    else:\n",
    "        pred = model.predict(n=1, series=list(train_series_dict.values()),num_samples=1)\n",
    "    last_values = {\n",
    "        key: ts.values()[-1].flatten()[0]\n",
    "        for key, ts in valid_series_dict.items()\n",
    "    }\n",
    "    # Extract the predicted values from pred (assuming the order matches valid_series_dict keys)\n",
    "    pred_values = [ts.values().flatten()[0] for ts in pred]\n",
    "    # Initialize lists to store actual and predicted values\n",
    "    actual_values = []\n",
    "    predicted_values = []\n",
    "    keys = []\n",
    "\n",
    "    # Extract actual and predicted values\n",
    "    for (key, last_value), pred_value in zip(last_values.items(), pred_values):\n",
    "        actual_values.append(last_value)\n",
    "        predicted_values.append(pred_value)\n",
    "        keys.append(key)\n",
    "\n",
    "    # Convert lists to numpy arrays for easier computation\n",
    "    actual_values_np = np.array(actual_values)\n",
    "    predicted_values_np = np.array(predicted_values)\n",
    "\n",
    "    # Calculate MAE\n",
    "    mae = np.mean(np.abs(actual_values_np - predicted_values_np))\n",
    "    mse = np.mean((actual_values_np - predicted_values_np) ** 2)\n",
    "    # Calculate RMSE\n",
    "    rmse = np.sqrt(np.mean((actual_values_np - predicted_values_np) ** 2))\n",
    "\n",
    "    # Calculate MAPE\n",
    "    mape = np.mean(np.abs((actual_values_np - predicted_values_np) / actual_values_np)) * 100\n",
    "    denominator = np.abs(actual_values_np) + np.abs(predicted_values_np)\n",
    "    denominator[denominator == 0] = 1\n",
    "    smape = np.mean(np.abs(actual_values_np - predicted_values_np) / denominator) * 100\n",
    "\n",
    "    # Print the metrics\n",
    "    print(f\"MAE: {mae}\")\n",
    "    print(f\"MASE: {mse}\")\n",
    "    print(f\"RMSE: {rmse}\")\n",
    "    #print(f\"MAPE: {mape}%\")\n",
    "\n",
    "    print(f\"SMAPE: {smape}%\")\n",
    "    return predicted_values, actual_values, keys\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict(model, train_series_dict, valid_series_dict, covariates, train_series_covars):\n",
    "    if covariates == True:\n",
    "        pred = model.predict(n=1, series=list(train_series_dict.values()), past_covariates=list(train_series_covars.values()))\n",
    "    else:\n",
    "        pred = model.predict(n=1, series=list(train_series_dict.values()), num_samples=5)\n",
    "\n",
    "    # Extract the last actual values from the validation series\n",
    "    last_values = {\n",
    "        key: ts.values()[-1].flatten()[0]\n",
    "        for key, ts in valid_series_dict.items()\n",
    "    }\n",
    "\n",
    "    # Extract the predicted values from pred (assuming the order matches valid_series_dict keys)\n",
    "    # Calculate the median of the predictions (axis 0 is the time dimension, axis 1 is the sample dimension)\n",
    "    pred_medians = [np.median(ts.values(), axis=1)[0] for ts in pred]\n",
    "\n",
    "    # Initialize lists to store actual and predicted values\n",
    "    actual_values = []\n",
    "    predicted_values = []\n",
    "    keys = []\n",
    "\n",
    "    # Extract actual and predicted values\n",
    "    for (key, last_value), pred_median in zip(last_values.items(), pred_medians):\n",
    "        actual_values.append(last_value)\n",
    "        predicted_values.append(pred_median)\n",
    "        keys.append(key)\n",
    "\n",
    "    # Convert lists to numpy arrays for easier computation\n",
    "    actual_values_np = np.array(actual_values)\n",
    "    predicted_values_np = np.array(predicted_values)\n",
    "\n",
    "    # Calculate MAE\n",
    "    mae = np.mean(np.abs(actual_values_np - predicted_values_np))\n",
    "    mse = np.mean((actual_values_np - predicted_values_np) ** 2)\n",
    "    # Calculate RMSE\n",
    "    rmse = np.sqrt(np.mean((actual_values_np - predicted_values_np) ** 2))\n",
    "\n",
    "    # Calculate MAPE\n",
    "    mape = np.mean(np.abs((actual_values_np - predicted_values_np) / actual_values_np)) * 100\n",
    "    \n",
    "    # Calculate SMAPE\n",
    "    denominator = np.abs(actual_values_np) + np.abs(predicted_values_np)\n",
    "    denominator[denominator == 0] = 1  # To avoid division by zero\n",
    "    smape = np.mean(np.abs(actual_values_np - predicted_values_np) / denominator) * 100\n",
    "\n",
    "    # Print the metrics\n",
    "    print(f\"MAE: {mae}\")\n",
    "    print(f\"MSE: {mse}\")\n",
    "    print(f\"RMSE: {rmse}\")\n",
    "    print(f\"MAPE: {mape}%\")\n",
    "    print(f\"SMAPE: {smape}%\")\n",
    "\n",
    "    return predicted_values, actual_values, keys, pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_processing(data):\n",
    "    start_prediction,restrict_station,station,value_to_predict,resample_rate,prediction_length,max_train_length,min_train_length,min_ts_length,metric,imputation,persistence_use_case,persistence_time_lag,min_value,padding,covariates,covariates_list,model_library = config_load()\n",
    "    filtered_data, filtered_data_full = process_ts(data,start_prediction,value_to_predict,min_value,persistence_use_case,persistence_time_lag)\n",
    "    if restrict_station == True:\n",
    "        df = filter_station(filtered_data,station)\n",
    "        resampled_df = resample_ts(df,resample_rate,min_ts_length,imputation,max_train_length,value_to_predict,covariates)\n",
    "    else:\n",
    "        resampled_df = resample_ts(filtered_data,resample_rate,min_ts_length,imputation,max_train_length,value_to_predict,covariates)\n",
    "\n",
    "    unique_encounter_ids = resampled_df['encounter_id'].unique()\n",
    "    train_encounter_ids, test_encounter_ids = train_test_split(unique_encounter_ids, test_size=0.2, random_state=42)\n",
    "    train_df = resampled_df[resampled_df['encounter_id'].isin(train_encounter_ids)]\n",
    "    test_df = resampled_df[resampled_df['encounter_id'].isin(test_encounter_ids)]\n",
    "\n",
    "    valid_series_dict, test_series_dict, train_series_dict = ts_dictionary(train_df,prediction_length)\n",
    "    valid_series_covars, test_series_covars, train_series_covars = ts_dictionary_covariates(train_df, prediction_length,covariates_list)\n",
    "\n",
    "    valid_series_dict_test, test_series_dict_test, train_series_dict_test = ts_dictionary(test_df,prediction_length)\n",
    "    valid_series_covars_test, test_series_covars_test, train_series_covars_test = ts_dictionary_covariates(test_df, prediction_length,covariates_list)\n",
    "\n",
    "    valid_series_dict_full, test_series_dict_full, train_series_dict_full = ts_dictionary(resampled_df,prediction_length)\n",
    "    valid_series_covars_full, test_series_covars_full, train_series_covars_full = ts_dictionary_covariates(resampled_df, prediction_length,covariates_list)\n",
    "\n",
    "    model_nbeats, model_xgb, model_lgb, model_tide,model_aarima = model_configs(min_train_length,prediction_length, max_train_length,padding)\n",
    "\n",
    "    # Pad the series\n",
    "    if padding == True:\n",
    "        train_series_dict = pad_series_dict(train_series_dict, max_train_length, pad_value=0)\n",
    "        train_series_covars = pad_series_dict(train_series_covars, max_train_length, pad_value=0)\n",
    "        train_series_dict_test = pad_series_dict(train_series_dict_test, max_train_length, pad_value=0)\n",
    "        train_series_covars_test = pad_series_dict(train_series_covars_test, max_train_length, pad_value=0)\n",
    "        train_series_dict_full = pad_series_dict(train_series_dict_full, max_train_length, pad_value=0)\n",
    "        train_series_covars_full = pad_series_dict(train_series_covars_full, max_train_length, pad_value=0)   \n",
    "\n",
    "    return train_series_dict_full, train_series_covars_full,valid_series_dict_full,train_series_covars,resampled_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_processing_training(data,start_prediction,value_to_predict,min_value,persistence_use_case,persistence_time_lag,station,resample_rate,min_ts_length,imputation,max_train_length,prediction_length,covariates_list,min_train_length,padding,restrict_station, covariates):\n",
    "\n",
    "\n",
    "    filtered_data, filtered_data_full = process_ts(data,start_prediction,value_to_predict,min_value,persistence_use_case,persistence_time_lag)\n",
    "    if restrict_station == True:\n",
    "        df = filter_station(filtered_data,station)\n",
    "        resampled_df = resample_ts(df,resample_rate,min_ts_length,imputation,max_train_length,value_to_predict,covariates)\n",
    "    else:\n",
    "        resampled_df = resample_ts(filtered_data,resample_rate,min_ts_length,imputation,max_train_length,value_to_predict,covariates)\n",
    "\n",
    "    unique_encounter_ids = resampled_df['encounter_id'].unique()\n",
    "    train_encounter_ids, test_encounter_ids = train_test_split(unique_encounter_ids, test_size=0.2, random_state=42)\n",
    "    train_df = resampled_df[resampled_df['encounter_id'].isin(train_encounter_ids)]\n",
    "    test_df = resampled_df[resampled_df['encounter_id'].isin(test_encounter_ids)]\n",
    "\n",
    "    valid_series_dict, test_series_dict, train_series_dict = ts_dictionary(train_df,prediction_length)\n",
    "    valid_series_covars, test_series_covars, train_series_covars = ts_dictionary_covariates(train_df, prediction_length,covariates_list)\n",
    "\n",
    "    valid_series_dict_test, test_series_dict_test, train_series_dict_test = ts_dictionary(test_df,prediction_length)\n",
    "    valid_series_covars_test, test_series_covars_test, train_series_covars_test = ts_dictionary_covariates(test_df, prediction_length,covariates_list)\n",
    "\n",
    "    valid_series_dict_full, test_series_dict_full, train_series_dict_full = ts_dictionary(resampled_df,prediction_length)\n",
    "    valid_series_covars_full, test_series_covars_full, train_series_covars_full = ts_dictionary_covariates(resampled_df, prediction_length,covariates_list)\n",
    "\n",
    "    model_nbeats, model_xgb, model_lgb, model_tide,model_aarima = model_configs(min_train_length,prediction_length, max_train_length,padding)\n",
    "\n",
    "    # Pad the series\n",
    "    if padding == True:\n",
    "        train_series_dict = pad_series_dict(train_series_dict, max_train_length, pad_value=0)\n",
    "        train_series_covars = pad_series_dict(train_series_covars, max_train_length, pad_value=0)\n",
    "        train_series_dict_test = pad_series_dict(train_series_dict_test, max_train_length, pad_value=0)\n",
    "        train_series_covars_test = pad_series_dict(train_series_covars_test, max_train_length, pad_value=0)\n",
    "        train_series_dict_full = pad_series_dict(train_series_dict_full, max_train_length, pad_value=0)\n",
    "        train_series_covars_full = pad_series_dict(train_series_covars_full, max_train_length, pad_value=0)   \n",
    "\n",
    "    return train_series_dict_full, train_series_covars_full,valid_series_dict_full, train_series_dict,train_series_covars,train_series_dict_test,train_series_covars_test,valid_series_dict,valid_series_dict_test,valid_series_covars,valid_series_covars_test,model_nbeats, model_xgb, model_lgb, model_tide,model_aarima,resampled_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from csv files\n",
    "def load_data_from_csv_and_merge_no_covars(df_crp,df_ab_groups):\n",
    "    try:\n",
    "        # Dropping NaN values from specific dataframes\n",
    "        #df_bili = df_bili.dropna()\n",
    "        df_crp = df_crp.dropna()\n",
    "        #df_ggt = df_ggt.dropna()\n",
    "        #df_hb = df_hb.dropna()\n",
    "        #df_krea = df_krea.dropna()\n",
    "        #df_leua = df_leua.dropna()\n",
    "        #df_temp = df_temp.dropna()\n",
    "        # List of dataframes\n",
    "        dataframes = [df_crp]\n",
    "\n",
    "        def process_timestamps(df, time_column='recorded_time'):\n",
    "            try:\n",
    "                df[time_column] = pd.to_datetime(df[time_column], utc=True)\n",
    "                df['recorded_date'] = df[time_column].dt.date\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing timestamps for dataframe: {e}\")\n",
    "\n",
    "        # Process timestamps for each dataframe\n",
    "        for df in dataframes:\n",
    "            process_timestamps(df)\n",
    "\n",
    "        try:\n",
    "            df_ab_groups['recorded_time'] = pd.to_datetime(df_ab_groups['recorded_time'], utc=True)\n",
    "            df_ab_grouped = df_ab_groups.groupby('encounter_id')['recorded_time'].agg(['min', 'max']).reset_index()\n",
    "            df_ab_grouped.columns = ['encounter_id', 'min_timestamp', 'max_timestamp']\n",
    "            df_crp = pd.merge(df_crp, df_ab_grouped, on='encounter_id', how='left')\n",
    "            df_crp['intervention'] = ((df_crp['recorded_time'] >= df_crp['min_timestamp']) &\n",
    "                                       (df_crp['recorded_time'] <= df_crp['max_timestamp'])).astype(int)\n",
    "            df_crp = df_crp.drop(columns=['min_timestamp', 'max_timestamp'])\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing df_ab_groups: {e}\")\n",
    "\n",
    "        # Rename values\n",
    "        try:\n",
    "            df_crp = df_crp.rename(columns={'value': 'crp'})\n",
    "        except Exception as e:\n",
    "            print(f\"Error renaming columns: {e}\")\n",
    "\n",
    "        return df_crp, df_ab_groups\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create time lags\n",
    "def create_merge_no_covars(df_crp, df_ab_groups):\n",
    "    try:\n",
    "        # Convert 'recorded_time' column to datetime if it's not already\n",
    "        df_crp['recorded_time'] = pd.to_datetime(df_crp['recorded_time'], utc=True)\n",
    "        \n",
    "        # Group by 'encounter_id'\n",
    "        grouped = df_crp.groupby('encounter_id')\n",
    "\n",
    "        # Define functions to create time lag indicators\n",
    "        def within_24_hours(group):\n",
    "            try:\n",
    "                intervention_1_time = group.loc[group['intervention'] == 1, 'recorded_time'].min()\n",
    "                return (group['recorded_time'] >= intervention_1_time) & ((group['recorded_time'] - intervention_1_time).dt.total_seconds() <= 24*60*60)\n",
    "            except Exception as e:\n",
    "                print(f\"Error in within_24_hours: {e}\")\n",
    "                return pd.Series([False]*len(group), index=group.index)\n",
    "\n",
    "        def within_24_to_48_hours(group):\n",
    "            try:\n",
    "                intervention_1_time = group.loc[group['intervention'] == 1, 'recorded_time'].min()\n",
    "                return ((group['recorded_time'] - intervention_1_time).dt.total_seconds() > 24*60*60) & ((group['recorded_time'] - intervention_1_time).dt.total_seconds() <= 48*60*60)\n",
    "            except Exception as e:\n",
    "                print(f\"Error in within_24_to_48_hours: {e}\")\n",
    "                return pd.Series([False]*len(group), index=group.index)\n",
    "\n",
    "        def within_48_to_72_hours(group):\n",
    "            try:\n",
    "                intervention_1_time = group.loc[group['intervention'] == 1, 'recorded_time'].min()\n",
    "                return ((group['recorded_time'] - intervention_1_time).dt.total_seconds() > 48*60*60) & ((group['recorded_time'] - intervention_1_time).dt.total_seconds() <= 72*60*60)\n",
    "            except Exception as e:\n",
    "                print(f\"Error in within_48_to_72_hours: {e}\")\n",
    "                return pd.Series([False]*len(group), index=group.index)\n",
    "\n",
    "        def prior_to_24_hours(group):\n",
    "            try:\n",
    "                intervention_1_time = group.loc[group['intervention'] == 1, 'recorded_time'].min()\n",
    "                prior_intervention_time = intervention_1_time - pd.Timedelta(hours=24)\n",
    "                return (group['recorded_time'] >= prior_intervention_time) & (group['recorded_time'] < intervention_1_time)\n",
    "            except Exception as e:\n",
    "                print(f\"Error in prior_to_24_hours: {e}\")\n",
    "                return pd.Series([False]*len(group), index=group.index)\n",
    "\n",
    "        # Apply the functions to create new columns\n",
    "        try:\n",
    "            df_crp['time_lag_2'] = grouped.apply(within_24_hours).reset_index(level=0, drop=True)\n",
    "            df_crp['time_lag_1'] = grouped.apply(within_24_to_48_hours).reset_index(level=0, drop=True)\n",
    "            df_crp['time_lag_target'] = grouped.apply(within_48_to_72_hours).reset_index(level=0, drop=True)\n",
    "            time_lag_prior_24_hours_series = grouped.apply(prior_to_24_hours).reset_index(level=0, drop=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating time lag columns: {e}\")\n",
    "\n",
    "\n",
    "        # Base dataframe\n",
    "        base_df = df_crp[['encounter_id', 'subject_reference', 'recorded_time', 'crp', 'intervention', 'time_lag_1', 'time_lag_2', 'time_lag_target']].sort_values('recorded_time')\n",
    "        #base_df = df_crp[['encounter_id', 'subject_reference', 'recorded_time', 'crp', 'intervention', 'time_lag_1', 'time_lag_2', 'time_lag_target']].sort_values('recorded_time')\n",
    "\n",
    "        data = base_df\n",
    "        \n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred in create_time_lags: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "def plot_random_ts(random_seed,predicted_values,actual_values,keys,valid_series_dict):\n",
    "    all_values = list(zip(keys, predicted_values, actual_values))\n",
    "    if random_seed != None:\n",
    "        random.seed(random_seed)\n",
    "    else:\n",
    "        seed = int(time.time())  # Current time in seconds since the epoch\n",
    "        random.seed(seed)\n",
    "    all_identifiers = [entry[0] for entry in all_values]\n",
    "    random_identifier = random.choice(all_identifiers)\n",
    "    # Function to find the entry by identifier\n",
    "    def find_entry_by_identifier(values_list, search_id):\n",
    "        for entry in values_list:\n",
    "            if entry[0] == search_id:\n",
    "                return entry\n",
    "        return None  # Return None if the identifier is not found\n",
    "    \n",
    "    # Find the entry\n",
    "    result = find_entry_by_identifier(all_values, random_identifier)\n",
    "    #print(result)\n",
    "    specific_entry = valid_series_dict[result[0]]\n",
    "    df = specific_entry.pd_dataframe()\n",
    "    df = df.reset_index()\n",
    "    # Extract the additional point\n",
    "    additional_time = df['recorded_time'].iloc[-1]  # Last value in the recorded_time column\n",
    "    additional_crp = result[1]\n",
    "    plot_title = result[0]\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(df['recorded_time'], df['crp'], marker='o', linestyle='-', color='black', label='CRP Data')\n",
    "    plt.scatter(additional_time, additional_crp, color='red', edgecolor='black', s=100, label='Additional Point')  # Adding the extra point\n",
    "    plt.xlabel('Recorded Time')\n",
    "    plt.ylabel('CRP')\n",
    "    plt.title(plot_title)\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend()  # Add a legend to distinguish between data series\n",
    "    plt.tight_layout()  # Adjusts the plot to fit everything nicely\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_random_ts_line(random_seed, predicted_values, actual_values, keys, valid_series_dict):\n",
    "    all_values = list(zip(keys, predicted_values, actual_values))\n",
    "    if random_seed is not None:\n",
    "        random.seed(random_seed)\n",
    "    else:\n",
    "        seed = int(time.time())  # Current time in seconds since the epoch\n",
    "        random.seed(seed)\n",
    "    all_identifiers = [entry[0] for entry in all_values]\n",
    "    random_identifier = random.choice(all_identifiers)\n",
    "    \n",
    "    # Function to find the entry by identifier\n",
    "    def find_entry_by_identifier(values_list, search_id):\n",
    "        for entry in values_list:\n",
    "            if entry[0] == search_id:\n",
    "                return entry\n",
    "        return None  # Return None if the identifier is not found\n",
    "    \n",
    "    # Find the entry\n",
    "    result = find_entry_by_identifier(all_values, random_identifier)\n",
    "    \n",
    "    specific_entry = valid_series_dict[result[0]]\n",
    "    df = specific_entry.pd_dataframe()\n",
    "    df = df.reset_index()\n",
    "    \n",
    "    # Extract the additional point\n",
    "    additional_time = df['recorded_time'].iloc[-1]  # Last value in the recorded_time column\n",
    "    additional_crp = result[1]\n",
    "    plot_title = result[0]\n",
    "    \n",
    "    # Penultimate point\n",
    "    penultimate_time = df['recorded_time'].iloc[-2]\n",
    "    \n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(df['recorded_time'], df['crp'], marker='o', linestyle='-', color='black', label='CRP Data')\n",
    "    plt.scatter(additional_time, additional_crp, color='red', edgecolor='black', s=100, label='Additional Point')  # Adding the extra point\n",
    "    \n",
    "    # Add the vertical dotted blue line at the penultimate point\n",
    "    plt.axvline(x=penultimate_time, color='blue', linestyle='--', label='Start of antibiotic therapy')\n",
    "    \n",
    "    plt.xlabel('Recorded Time')\n",
    "    plt.ylabel('CRP')\n",
    "    plt.title(plot_title)\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend()  # Add a legend to distinguish between data series\n",
    "    plt.tight_layout()  # Adjusts the plot to fit everything nicely\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "def plot_random_ts_proba(random_seed, predicted_values, actual_values, keys, valid_series_dict):\n",
    "    all_values = list(zip(keys, predicted_values, actual_values))\n",
    "    if random_seed is not None:\n",
    "        random.seed(random_seed)\n",
    "    else:\n",
    "        seed = int(time.time())  # Current time in seconds since the epoch\n",
    "        random.seed(seed)\n",
    "    all_identifiers = [entry[0] for entry in all_values]\n",
    "    random_identifier = random.choice(all_identifiers)\n",
    "    \n",
    "    # Function to find the entry by identifier\n",
    "    def find_entry_by_identifier(values_list, search_id):\n",
    "        for entry in values_list:\n",
    "            if entry[0] == search_id:\n",
    "                return entry\n",
    "        return None  # Return None if the identifier is not found\n",
    "    \n",
    "    # Find the entry\n",
    "    result = find_entry_by_identifier(all_values, random_identifier)\n",
    "    \n",
    "    specific_entry = valid_series_dict[result[0]]\n",
    "    df = specific_entry.pd_dataframe()\n",
    "    df = df.reset_index()\n",
    "    \n",
    "    # Extract the additional point\n",
    "    additional_time = df['recorded_time'].iloc[-1]  # Last value in the recorded_time column\n",
    "    additional_crp = result[1]['q0.5']\n",
    "    plot_title = result[0]\n",
    "    \n",
    "    # Penultimate point\n",
    "    penultimate_time = df['recorded_time'].iloc[-2]\n",
    "    \n",
    "    # Extract actual and predicted values for the specific series\n",
    "    actual_series_values = df['crp'].values\n",
    "    \n",
    "    # Extract predicted quantiles\n",
    "    predicted_series_q05 = result[1]['q0.05']\n",
    "    predicted_series_q50 = result[1]['q0.5']\n",
    "    predicted_series_q95 = result[1]['q0.95']\n",
    "\n",
    "    # Ensure predicted_series_values are array-like structures\n",
    "    predicted_series_q05 = np.array(predicted_series_q05)\n",
    "    predicted_series_q50 = np.array(predicted_series_q50)\n",
    "    predicted_series_q95 = np.array(predicted_series_q95)\n",
    "\n",
    "    # Calculate MAE for the 0.5 quantile\n",
    "    mae = mean_absolute_error(actual_series_values, predicted_series_q50)\n",
    "    \n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(df['recorded_time'], df['crp'], marker='o', linestyle='-', color='black', label='CRP Data')\n",
    "    plt.scatter(df['recorded_time'].iloc[-1], additional_crp, color='red', edgecolor='black', s=100, label='Additional Point')  # Adding the extra point\n",
    "    \n",
    "    # Add the vertical dotted blue line at the penultimate point\n",
    "    plt.axvline(x=penultimate_time, color='blue', linestyle='--', label='Start of antibiotic therapy')\n",
    "    \n",
    "    # Plot the 0.5 quantile as points\n",
    "    plt.scatter(df['recorded_time'], predicted_series_q50, color='green', label='Predicted q0.5')\n",
    "    \n",
    "    # Plot the 0.05 and 0.95 quantiles as a shaded area\n",
    "    plt.fill_between(df['recorded_time'], predicted_series_q05, predicted_series_q95, color='orange', alpha=0.3, label='0.05-0.95 interval')\n",
    "    \n",
    "    plt.xlabel('Recorded Time')\n",
    "    plt.ylabel('CRP')\n",
    "    plt.title(f\"{plot_title} (MAE: {mae:.2f})\")  # Display MAE in the title\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend()  # Add a legend to distinguish between data series\n",
    "    plt.tight_layout()  # Adjusts the plot to fit everything nicely\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "# plot_random_ts(random_seed, predicted_values, actual_values, keys, valid_series_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_ts_key(selected_key, predicted_values, actual_values, keys, valid_series_dict):\n",
    "    all_values = list(zip(keys, predicted_values, actual_values))\n",
    "    \n",
    "    # Function to find the entry by identifier\n",
    "    def find_entry_by_identifier(values_list, search_id):\n",
    "        for entry in values_list:\n",
    "            if entry[0] == search_id:\n",
    "                return entry\n",
    "        return None  # Return None if the identifier is not found\n",
    "    \n",
    "    # Find the entry\n",
    "    result = find_entry_by_identifier(all_values, selected_key)\n",
    "    if result is None:\n",
    "        raise ValueError(f\"Key '{selected_key}' not found in the provided data.\")\n",
    "    \n",
    "    specific_entry = valid_series_dict[result[0]]\n",
    "    df = specific_entry.pd_dataframe()\n",
    "    df = df.reset_index()\n",
    "    \n",
    "    # Extract the additional point\n",
    "    additional_time = df['recorded_time'].iloc[-1]  # Last value in the recorded_time column\n",
    "    additional_crp = result[1]\n",
    "    plot_title = result[0]\n",
    "    \n",
    "    # Penultimate point\n",
    "    penultimate_time = df['recorded_time'].iloc[-2]\n",
    "    \n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(df['recorded_time'], df['crp'], marker='o', linestyle='-', color='black', label='CRP Data')\n",
    "    plt.scatter(additional_time, additional_crp, color='red', edgecolor='black', s=100, label='Additional Point')  # Adding the extra point\n",
    "    \n",
    "    # Add the vertical dotted blue line at the penultimate point\n",
    "    plt.axvline(x=penultimate_time, color='blue', linestyle='--', label='Start of antibiotic therapy')\n",
    "    \n",
    "    plt.xlabel('Recorded Time')\n",
    "    plt.ylabel('CRP')\n",
    "    plt.title(plot_title)\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend()  # Add a legend to distinguish between data series\n",
    "    plt.tight_layout()  # Adjusts the plot to fit everything nicely\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def predict_proba(model, train_series_dict, valid_series_dict, covariates, train_series_covars):\n",
    "    if covariates:\n",
    "        pred = model.predict(n=1, series=list(train_series_dict.values()), past_covariates=list(train_series_covars.values()))\n",
    "    else:\n",
    "        pred = model.predict(n=1, series=list(train_series_dict.values()), num_samples=500)\n",
    "\n",
    "    pred_quantile = ([ts.quantile(0.5) for ts in pred])\n",
    "    # Extract the last actual values from the validation series\n",
    "    last_values = {\n",
    "        key: ts.values()[-1].flatten()[0]\n",
    "        for key, ts in valid_series_dict.items()\n",
    "    }\n",
    "\n",
    "    # Convert the list of TimeSeries to a NumPy array\n",
    "    pred_arrays = np.array([ts.values() for ts in pred_quantile])  # Shape should be (num_series, 1, num_samples)\n",
    "    #pred_arrays = ([ts.values() for ts in pred])\n",
    "    # Debugging output\n",
    "    #print(f\"Shape of pred_arrays: {pred_arrays.shape}\")  # Debugging line\n",
    "\n",
    "    # Calculate the median prediction for each series\n",
    "    if pred_arrays.ndim == 3:\n",
    "    #    # Take the median across the sample dimension (axis 2)\n",
    "        pred_medians = np.median(pred_arrays, axis=2)[:, 0]  # Shape (num_series,)\n",
    "    else:\n",
    "    #    # Handle case where num_samples is 1\n",
    "        pred_medians = np.squeeze(pred_arrays)  # Shape (num_series,)\n",
    "\n",
    "    # Initialize lists to store actual and predicted values\n",
    "    actual_values = []\n",
    "    predicted_values = []\n",
    "    keys = []\n",
    "\n",
    "    # Extract actual and predicted values\n",
    "    for (key, last_value), pred_arrays in zip(last_values.items(), pred_medians):\n",
    "        actual_values.append(last_value)\n",
    "        predicted_values.append(pred_medians)\n",
    "        keys.append(key)\n",
    "\n",
    "    # Convert lists to numpy arrays for easier computation\n",
    "    actual_values_np = np.array(actual_values)\n",
    "    predicted_values_np = np.array(predicted_values)\n",
    "\n",
    "    # Calculate MAE\n",
    "    mae = np.mean(np.abs(actual_values_np - predicted_values_np))\n",
    "    mse = np.mean((actual_values_np - predicted_values_np) ** 2)\n",
    "    # Calculate RMSE\n",
    "    rmse = np.sqrt(np.mean((actual_values_np - predicted_values_np) ** 2))\n",
    "\n",
    "    # Calculate MAPE\n",
    "    mape = np.mean(np.abs((actual_values_np - predicted_values_np) / actual_values_np)) * 100\n",
    "    \n",
    "    # Calculate SMAPE\n",
    "    denominator = np.abs(actual_values_np) + np.abs(predicted_values_np)\n",
    "    denominator[denominator == 0] = 1  # To avoid division by zero\n",
    "    smape = np.mean(np.abs(actual_values_np - predicted_values_np) / denominator) * 100\n",
    "\n",
    "    # Print the metrics\n",
    "    print(f\"MAE: {mae}\")\n",
    "    print(f\"MSE: {mse}\")\n",
    "    print(f\"RMSE: {rmse}\")\n",
    "    print(f\"MAPE: {mape}%\")\n",
    "    print(f\"SMAPE: {smape}%\")\n",
    "    pred_ci_down = ([ts.quantile(0.05) for ts in pred])\n",
    "    pred_ci_up = ([ts.quantile(0.95) for ts in pred])\n",
    "    pred_ci_down = np.array([ts.values() for ts in pred_ci_down])\n",
    "    pred_ci_up = np.array([ts.values() for ts in pred_ci_up])\n",
    "    return predicted_values, actual_values, keys, pred_quantile,pred_ci_down,pred_ci_up\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def plot_random_ts_proba(key_index, predicted_values, actual_values, keys, valid_series_dict_full, pred_ci_up, pred_ci_down):\n",
    "    if key_index == None:\n",
    "        #key_index = random.choice(keys)\n",
    "        key_index = random.randint(0, len(keys) - 1)\n",
    "        print(key_index)\n",
    "    predicted_values = [value for array in predicted_values for value in array]\n",
    "   # Extract the entry corresponding to the specified key index\n",
    "    key = keys[key_index]\n",
    "    predicted_value = predicted_values[key_index]\n",
    "    actual_value = actual_values[key_index]\n",
    "\n",
    "    specific_entry = valid_series_dict_full[key]\n",
    "    df = specific_entry.pd_dataframe()\n",
    "    df = df.reset_index()\n",
    "\n",
    "    # Extract the additional point\n",
    "    additional_time = df['recorded_time'].iloc[-1]  # Last value in the recorded_time column\n",
    "    additional_crp = predicted_value\n",
    "    plot_title = \"Recorded and predicted CRP values for ID \"+key\n",
    "\n",
    "    # Extract the index of the additional point for CI values\n",
    "    additional_index = df.index[df['recorded_time'] == additional_time].tolist()[0]\n",
    "\n",
    "    additional_ci_up = pred_ci_up[key_index][0][0]\n",
    "    additional_ci_down2 = pred_ci_down[key_index][0][0]\n",
    "    # Penultimate point\n",
    "    penultimate_time = df['recorded_time'].iloc[-2]\n",
    "    mae = np.mean(np.abs(actual_value - predicted_value))\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(df['recorded_time'], df['crp'], marker='o', linestyle='-', color='black', label='Recorded CRP values')\n",
    "    plt.scatter(additional_time, additional_crp, color='red', edgecolor='black', s=100, label='Predicted CRP value')  # Adding the extra point\n",
    "    plt.errorbar(additional_time, additional_crp, yerr=[[additional_crp-additional_ci_down2], [additional_ci_up-additional_crp]], fmt='o', color='red', capsize=5, label='95% CI')\n",
    "\n",
    "    # Add the vertical dotted blue line at the penultimate point\n",
    "    plt.axvline(x=penultimate_time, color='blue', linestyle='--', label='Start of antibiotic therapy')\n",
    "\n",
    "    plt.xlabel('Recorded Time')\n",
    "    plt.ylabel('CRP')\n",
    "    #plt.title(plot_title)\n",
    "    plt.title(f\"{key} (MAE: {mae:.2f})\")  # Display MAE in the title\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend()  # Add a legend to distinguish between data series\n",
    "    plt.tight_layout()  # Adjusts the plot to fit everything nicely\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show() \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def config_load():\n",
    "    # Load configuration from YAML-file\n",
    "    # No changes to configuration.yaml are required for executing the pretrained models.\n",
    "    with open('config.yaml', 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "\n",
    "    # Access configuration values\n",
    "    start_prediction = config['model_configurations']['start_prediction']\n",
    "    restrict_station = config['model_configurations']['restrict_station']\n",
    "    station = config['model_configurations']['station']\n",
    "    value_to_predict = config['model_configurations']['value_to_predict']\n",
    "    resample_rate = config['model_configurations']['resample_rate']\n",
    "    prediction_length = config['model_configurations']['prediction_length']\n",
    "    max_train_length = config['model_configurations']['max_train_length']\n",
    "    min_train_length = config['model_configurations']['min_train_length']\n",
    "    min_ts_length = config['model_configurations']['min_ts_length']\n",
    "    metric = config['model_configurations']['metric']\n",
    "    imputation = config['model_configurations']['imputation']\n",
    "    persistence_use_case = config['model_configurations']['persistence_use_case']\n",
    "    persistence_time_lag = config['model_configurations']['persistence_time_lag']\n",
    "    min_value = config['model_configurations']['min_value']\n",
    "    padding = config['model_configurations']['padding']\n",
    "    covariates = config['model_configurations']['covariates']\n",
    "    covariates_list = config['model_configurations']['covariates_list']\n",
    "    model_library = config['model_configurations']['model_library']\n",
    "    return start_prediction,restrict_station,station,value_to_predict,resample_rate,prediction_length,max_train_length,min_train_length,min_ts_length,metric,imputation,persistence_use_case,persistence_time_lag,min_value,padding,covariates,covariates_list,model_library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autogluon_data(resampled_df,predictor):\n",
    "    with open('config.yaml', 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "    prediction_length = config['model_configurations']['prediction_length']\n",
    "    value_to_predict = config['model_configurations']['value_to_predict']\n",
    "\n",
    "    # Split the data into train and test sets\n",
    "    def split_train_test(group):\n",
    "        test_rows = group.nlargest(prediction_length, 'recorded_time')\n",
    "        train_rows = group.drop(test_rows.index)\n",
    "        return train_rows, test_rows\n",
    "    \n",
    "    train_list = []\n",
    "    test_list = []\n",
    "    \n",
    "    # Group by encounter_id and split each group\n",
    "    for name, group in resampled_df.groupby('encounter_id'):\n",
    "        train_rows, test_rows = split_train_test(group)\n",
    "        train_list.append(train_rows)\n",
    "        test_list.append(test_rows)\n",
    "    \n",
    "    # Concatenate the lists into DataFrames\n",
    "    train_data = pd.concat(train_list).reset_index(drop=True)\n",
    "    test_data = pd.concat(test_list).reset_index(drop=True)\n",
    "    train_data_ = train_data[['encounter_id', 'recorded_time', value_to_predict]]\n",
    "    train_data_['recorded_time'] = train_data_['recorded_time'].dt.tz_localize(None)\n",
    "    test_data_ = resampled_df[['encounter_id', 'recorded_time', value_to_predict]]\n",
    "    test_data_['recorded_time'] = test_data_['recorded_time'].dt.tz_localize(None)\n",
    "    train_data = TimeSeriesDataFrame.from_data_frame(\n",
    "        train_data_,\n",
    "        id_column=\"encounter_id\",\n",
    "        timestamp_column=\"recorded_time\"\n",
    "    )\n",
    "    test_data = TimeSeriesDataFrame.from_data_frame(\n",
    "        test_data_,\n",
    "        id_column=\"encounter_id\",\n",
    "        timestamp_column=\"recorded_time\"\n",
    "    )\n",
    "    predictions = predictor.predict(train_data,model='WeightedEnsemble')\n",
    "    predictions_ = predictions[['mean','0.05','0.95']]\n",
    "    merged_data = test_data.join(predictions_, how=\"inner\")\n",
    "    # Extract actual and predicted values\n",
    "    actuals = merged_data['crp'].values\n",
    "    predictions = merged_data['mean'].values\n",
    "\n",
    "    # MAE: Mean Absolute Error\n",
    "    mae = mean_absolute_error(actuals, predictions)\n",
    "\n",
    "    # MSE: Mean Squared Error\n",
    "    mse = mean_squared_error(actuals, predictions)\n",
    "\n",
    "    # RMSE: Root Mean Squared Error\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    # MAPE: Mean Absolute Percentage Error\n",
    "    mape = mean_absolute_percentage_error(actuals, predictions) * 100\n",
    "\n",
    "    # SMAPE: Symmetric Mean Absolute Percentage Error\n",
    "    smape = np.mean(2 * np.abs(actuals - predictions) / (np.abs(actuals) + np.abs(predictions))) * 100\n",
    "\n",
    "    # Display the results\n",
    "    print(f\"MAE: {mae:.4f}\")\n",
    "    print(f\"MSE: {mse:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    print(f\"MAPE: {mape:.4f}%\")\n",
    "    print(f\"SMAPE: {smape:.4f}%\")\n",
    "\n",
    "    return predictions,train_data, test_data, merged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_random_ts_proba_gluon(random_seed, test_data, merged_data):\n",
    "    # Get the unique identifiers (first level of MultiIndex)\n",
    "    all_identifiers = merged_data.index.get_level_values(0).unique()\n",
    "    \n",
    "    # Set random seed\n",
    "    if random_seed is not None:\n",
    "        random.seed(random_seed)\n",
    "    else:\n",
    "        seed = int(time.time())\n",
    "        random.seed(seed)\n",
    "\n",
    "    # Choose a random identifier from merged_data (where predictions exist)\n",
    "    random_identifier = random.choice(all_identifiers)\n",
    "\n",
    "    # Filter the historical data for the selected identifier from test_data\n",
    "    historic_data = test_data.loc[random_identifier]\n",
    "\n",
    "    # Filter the prediction data for the selected identifier from merged_data\n",
    "    prediction_data = merged_data.loc[random_identifier]\n",
    "\n",
    "    # Extract the time series data for the chosen identifier\n",
    "    recorded_time = historic_data.index\n",
    "    actual_series_values = historic_data['crp'].values\n",
    "\n",
    "    # Extract prediction and confidence intervals\n",
    "    prediction_time = prediction_data.index\n",
    "    predicted_series_q05 = prediction_data['0.05'].values\n",
    "    predicted_series_q05 = np.maximum(predicted_series_q05, 0)\n",
    "    predicted_series_q50 = prediction_data['mean'].values\n",
    "    predicted_series_q95 = prediction_data['0.95'].values\n",
    "\n",
    "    # Calculate MAE for the 0.5 quantile\n",
    "    mae = mean_absolute_error(actual_series_values[-len(predicted_series_q50):], predicted_series_q50)\n",
    "\n",
    "    # Identify the penultimate point in the historical data\n",
    "    penultimate_time = recorded_time[-2]\n",
    "\n",
    "    # Extract the additional point (last predicted point)\n",
    "    additional_time = prediction_time[-1]\n",
    "    additional_crp = predicted_series_q50[-1]\n",
    "    additional_ci_down = predicted_series_q05[-1]\n",
    "    additional_ci_up = predicted_series_q95[-1]\n",
    "    \n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Plot the actual historical CRP data\n",
    "    plt.plot(recorded_time, actual_series_values, marker='o', linestyle='-', color='black', label='CRP Data')\n",
    "\n",
    "    # Add the vertical dotted blue line at the penultimate point\n",
    "    plt.axvline(x=penultimate_time, color='blue', linestyle='--', label='Start of antibiotic therapy')\n",
    "\n",
    "    # Plot the predicted q0.5 (median) as points\n",
    "    plt.scatter(prediction_time, predicted_series_q50, color='red', edgecolor='black', s=100, label='Predicted CRP value')\n",
    "\n",
    "    # Add the error bar for the last predicted point with 95% CI\n",
    "    plt.errorbar(additional_time, additional_crp, \n",
    "                 yerr=[[additional_crp - additional_ci_down], [additional_ci_up - additional_crp]], \n",
    "                 fmt='o', color='red', capsize=5, label='95% CI')\n",
    "    \n",
    "    plt.xlabel('Recorded Time')\n",
    "    plt.ylabel('CRP')\n",
    "    plt.title(f\"{random_identifier} (MAE: {mae:.2f})\")  # Display MAE in the title\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend()  # Add a legend to distinguish between data series\n",
    "    plt.tight_layout()  # Adjusts the plot to fit everything nicely\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
